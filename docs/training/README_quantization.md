# é‡å­åŒ–å­¦ç¿’ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆã‚¬ã‚¤ãƒ‰

TrackNetã®int8é‡å­åŒ–ã¨QLoRAæ©Ÿèƒ½ã‚’ã™ãã«ä½¿ã„å§‹ã‚ã‚‹ãŸã‚ã®ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚

## ğŸš€ ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

### 1. int8é‡å­åŒ–å­¦ç¿’

```bash
# åŸºæœ¬çš„ãªint8å­¦ç¿’
uv run python -m tracknet.scripts.train \
  --data tracknet \
  --model vit_heatmap \
  --training int8

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ã•ã‚‰ã«å‰Šæ¸›
uv run python -m tracknet.scripts.train \
  --data tracknet \
  --model vit_heatmap \
  --training int8 \
  training.batch_size=8 \
  training.memory_optimization.gradient_checkpointing=true
```

### 2. QLoRAå­¦ç¿’

```bash
# åŸºæœ¬çš„ãªQLoRAå­¦ç¿’
uv run python -m tracknet.scripts.train \
  --data tracknet \
  --model vit_heatmap \
  --training qlora

# LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º
uv run python -m tracknet.scripts.train \
  --data tracknet \
  --model vit_heatmap \
  --training qlora \
  training.lora.r=32 \
  training.lora.lora_alpha=64 \
  training.batch_size=4
```

### 3. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ä»˜ãQLoRA

```bash
# æœ€å¤§é™ã®ãƒ¡ãƒ¢ãƒªåŠ¹ç‡
uv run python -m tracknet.scripts.train \
  --data tracknet \
  --model vit_heatmap \
  --training qlora \
  training.memory_optimization.gradient_checkpointing=true \
  training.memory_optimization.use_cpu_offload=true \
  training.min_micro_batch_size=1 \
  training.batch_size=2
```

## ğŸ“Š ã©ã®æ‰‹æ³•ã‚’é¸ã¶ã¹ãã‹ï¼Ÿ

| GPUãƒ¡ãƒ¢ãƒª | æ¨å¥¨æ‰‹æ³• | ãƒãƒƒãƒã‚µã‚¤ã‚º | ç²¾åº¦ | é€Ÿåº¦ |
|-----------|----------|-------------|------|------|
| 16GB+ | é€šå¸¸å­¦ç¿’ (FP16) | 32 | 100% | åŸºæº– |
| 12GB | int8é‡å­åŒ– | 16 | 98-99% | 1.2x |
| 8GB | QLoRA | 8 | 97-99% | 0.8x |
| 4GB | QLoRA + CPUã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰ | 4 | 95-98% | 0.5x |

## âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å ´æ‰€

- `configs/training/int8.yaml` - int8é‡å­åŒ–è¨­å®š
- `configs/training/qlora.yaml` - QLoRAè¨­å®š
- `configs/training/default.yaml` - é€šå¸¸å­¦ç¿’è¨­å®š

## ğŸ”§ ã‚ˆãã‚ã‚‹ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

### ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´

```bash
# int8ã®å ´åˆ
training.batch_size=8  # 16â†’8ã«å‰Šæ¸›

# QLoRAã®å ´åˆ
training.batch_size=4  # 8â†’4ã«å‰Šæ¸›
```

### LoRAãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®èª¿æ•´

```bash
# ã‚ˆã‚Šé«˜ã„ç²¾åº¦ãŒå¿…è¦ãªå ´åˆ
training.lora.r=32
training.lora.lora_alpha=64

# ã‚ˆã‚Šå°‘ãªã„ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¿…è¦ãªå ´åˆ
training.lora.r=8
training.lora.lora_alpha=16
```

### å­¦ç¿’ç‡ã®èª¿æ•´

```bash
# int8ã®å ´åˆ
training.optimizer.lr=3e-4

# QLoRAã®å ´åˆ
training.optimizer.lr=1e-4
```

## ğŸ“ˆ ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°

å­¦ç¿’ã®é€²æ—ã¨ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’ç›£è¦–ï¼š

```bash
# TensorBoardã‚’èµ·å‹•
tensorboard --logdir=outputs/logs

# ç›£è¦–ã™ã‚‹ãƒ¡ãƒˆãƒªã‚¯ã‚¹:
# - train/loss: å­¦ç¿’æå¤±
# - memory/allocated_gb: ç¢ºä¿æ¸ˆã¿GPUãƒ¡ãƒ¢ãƒª
# - memory/cached_gb: ã‚­ãƒ£ãƒƒã‚·ãƒ¥GPUãƒ¡ãƒ¢ãƒª
```

## ğŸ†˜ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### OOMã‚¨ãƒ©ãƒ¼

```bash
# å¯¾ç­–1: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‰Šæ¸›
training.batch_size=4

# å¯¾ç­–2: ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚’æœ‰åŠ¹åŒ–
training.adaptive_micro_batch=true
training.min_micro_batch_size=1

# å¯¾ç­–3: ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–
training.memory_optimization.gradient_checkpointing=true
training.memory_optimization.use_cpu_offload=true
```

### ç²¾åº¦ãŒä½ä¸‹

```bash
# å¯¾ç­–1: LoRAãƒ©ãƒ³ã‚¯ã‚’å¢—ã‚„ã™
training.lora.r=32
training.lora.lora_alpha=64

# å¯¾ç­–2: å­¦ç¿’ç‡ã‚’èª¿æ•´
training.optimizer.lr=2e-4

# å¯¾ç­–3: ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å¢—ã‚„ã™
training.epochs=30
```

## ğŸ“ è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- [`quantization.md`](quantization.md) - è©³ç´°ãªæŠ€è¡“ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [`micro_batching.md`](micro_batching.md) - è‡ªå‹•ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒãƒ³ã‚°
- [`trainer.md`](trainer.md) - ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼è¨­å®š

## ğŸ¯ ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

### int8é‡å­åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1. **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 16ä»¥ä¸‹ã«è¨­å®š
2. **å­¦ç¿’ç‡**: 3e-4ç¨‹åº¦ã«èª¿æ•´
3. **ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—**: 2ã‚¨ãƒãƒƒã‚¯ç¨‹åº¦
4. **å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°**: 1.0ã‚’æ¨å¥¨

### QLoRAã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1. **ãƒãƒƒãƒã‚µã‚¤ã‚º**: 8ä»¥ä¸‹ã«è¨­å®š
2. **LoRAãƒ©ãƒ³ã‚¯**: 16-32ã®ç¯„å›²ã§èª¿æ•´
3. **å­¦ç¿’ç‡**: 1e-4ç¨‹åº¦ã«èª¿æ•´
4. **ã‚¨ãƒãƒƒã‚¯æ•°**: 30ä»¥ä¸Šã‚’æ¨å¥¨
5. **ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—**: 5ã‚¨ãƒãƒƒã‚¯ç¨‹åº¦

### å…±é€šã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

1. **æ··åˆç²¾åº¦**: fp16ã¾ãŸã¯bf16ã‚’ä½¿ç”¨
2. **ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒãƒ³ã‚°**: å¸¸ã«æœ‰åŠ¹åŒ–
3. **ãƒ¡ãƒ¢ãƒªç›£è¦–**: TensorBoardã§ç›£è¦–
4. **æ®µéšçš„ãªã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—**: å°ã•ã„è¨­å®šã‹ã‚‰å§‹ã‚ã‚‹

## ğŸ”„ ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿

### int8é‡å­åŒ–ãƒ¢ãƒ‡ãƒ«

```python
# ä¿å­˜
torch.save(quantized_model.state_dict(), "int8_model.pth")

# èª­ã¿è¾¼ã¿
model = build_model(model_cfg, training_cfg)
model.load_state_dict(torch.load("int8_model.pth"))
```

### QLoRAãƒ¢ãƒ‡ãƒ«

```python
# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ã‚’ä¿å­˜
model.save_pretrained("lora_adapters")

# èª­ã¿è¾¼ã¿
from peft import PeftModel
base_model = build_model(model_cfg)
model = PeftModel.from_pretrained(base_model, "lora_adapters")
```

## ğŸ“ ã‚µãƒãƒ¼ãƒˆ

å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆï¼š

1. ã¾ãšã“ã®ã‚¬ã‚¤ãƒ‰ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç¢ºèª
2. [`quantization.md`](quantization.md)ã®è©³ç´°ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§
3. GPUãƒ¡ãƒ¢ãƒªã¨ä¾å­˜é–¢ä¿‚ã‚’ç¢ºèª
4. å¿…è¦ã«å¿œã˜ã¦GitHubã§Issueã‚’ä½œæˆ

---

**Happy Training! ğŸ¾**

# int8量化トレーニング設定
# bitsandbytesを使用して8ビット量子化を有効化

# 基本設定
seed: 42
batch_size: 16  # int8の場合、バッチサイズを少し小さく設定
epochs: 20
precision: fp16  # 混合精度とint8量子化を組み合わせ

# int8量子化設定
quantization:
  enabled: true
  load_in_8bit: true
  llm_int8_threshold: 6.0
  llm_int8_has_fp16_weight: false
  llm_int8_skip_modules: []  # 量子化をスキップするモジュール（空ですべてを量子化）

# 勾配クリッピング
grad_clip: 1.0
backbone_freeze_epochs: 5  # 量子化の場合は凍結期間を短めに

# 適応的マイクロバッチング設定
adaptive_micro_batch: true
min_micro_batch_size: 2  # int8の場合はさらに小さく
mb_backoff_factor: 2
oom_retries: 3
micro_batch_size: 0
grad_clip_norm: 1.0

# DataLoaderパフォーマンスチューニング
data_loader:
  num_workers: 4
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 2
  drop_last: false

# オプティマイザ設定
optimizer:
  name: adamw
  lr: 3.0e-4  # int8の場合は学習率を少し低めに
  weight_decay: 0.01

# スケジューラ設定
scheduler:
  name: cosine
  warmup_epochs: 2
  warmup_start_factor: 0.1

# メモリ最適化
memory_optimization:
  gradient_checkpointing: true
  use_cpu_offload: false  # GPUメモリが十分な場合はfalse
  use_disk_offload: false

# 出力ディレクトリ
output_dir: outputs
log_dir: outputs/logs
ckpt_dir: outputs/checkpoints
